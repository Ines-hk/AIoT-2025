{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vo3KceTHHFso"
      },
      "outputs": [],
      "source": [
        "!pip install pandas dask psutil kaggle --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju53tgk3IrQv",
        "outputId": "b20123f7-ad51-4da6-cb3b-b0f0a0f418fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store\n",
            "License(s): copyright-authors\n",
            "Downloading ecommerce-behavior-data-from-multi-category-store.zip to /content\n",
            " 98% 4.21G/4.29G [01:11<00:05, 14.3MB/s]\n",
            "100% 4.29G/4.29G [01:11<00:00, 64.0MB/s]\n",
            "total 18G\n",
            "-rw-r--r-- 1 root root 8.4G Oct 15 09:18 2019-Nov.csv\n",
            "-rw-r--r-- 1 root root 5.3G Oct 15 09:19 2019-Oct.csv\n",
            "-rw-r--r-- 1 root root 4.3G Dec  9  2019 ecommerce-behavior-data-from-multi-category-store.zip\n",
            "drwxr-xr-x 1 root root 4.0K Oct 13 13:43 sample_data\n",
            "الملفات الموجودة: ['.config', '2019-Oct.csv', '2019-Nov.csv', 'ecommerce-behavior-data-from-multi-category-store.zip', 'sample_data']\n"
          ]
        }
      ],
      "source": [
        "# إعداد مفاتيح الوصول إلى Kaggle (مرة واحدة فقط)\n",
        "import os, zipfile\n",
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"ineshenka\"\n",
        "os.environ['KAGGLE_KEY'] = \"dbc5fb938f808148f7dd21506a40b2e5\"\n",
        "\n",
        "# تحميل ملف CSV من Kaggle\n",
        "# تغيير اسم مجموعة البيانات إلى مجموعة البيانات الجديدة\n",
        "!kaggle datasets download -d mkechinov/ecommerce-behavior-data-from-multi-category-store\n",
        "\n",
        "# فك الضغط عن الملف\n",
        "# تغيير اسم ملف zip حسب اسم ملف مجموعة البيانات الجديدة\n",
        "with zipfile.ZipFile(\"ecommerce-behavior-data-from-multi-category-store.zip\", \"r\") as z:\n",
        "    z.extractall(\".\")\n",
        "\n",
        "# التأكد من وجود الملف csv\n",
        "!ls -lh\n",
        "# التحقق من وجود الملف\n",
        "print(\"الملفات الموجودة:\", os.listdir(\".\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HeoT1SUKr1V",
        "outputId": "a904dd93-f59a-4cd3-8de0-f82233b9ad94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "معاينة أولية للبيانات:\n",
            "                event_time event_type  product_id          category_id  \\\n",
            "0  2019-10-01 00:00:00 UTC       view    44600062  2103807459595387724   \n",
            "1  2019-10-01 00:00:00 UTC       view     3900821  2053013552326770905   \n",
            "2  2019-10-01 00:00:01 UTC       view    17200506  2053013559792632471   \n",
            "3  2019-10-01 00:00:01 UTC       view     1307067  2053013558920217191   \n",
            "4  2019-10-01 00:00:04 UTC       view     1004237  2053013555631882655   \n",
            "\n",
            "                         category_code     brand    price    user_id  \\\n",
            "0                                  NaN  shiseido    35.79  541312140   \n",
            "1  appliances.environment.water_heater      aqua    33.20  554748717   \n",
            "2           furniture.living_room.sofa       NaN   543.10  519107250   \n",
            "3                   computers.notebook    lenovo   251.74  550050854   \n",
            "4               electronics.smartphone     apple  1081.98  535871217   \n",
            "\n",
            "                           user_session  \n",
            "0  72d76fde-8bb3-4e00-8c23-a032dfed738c  \n",
            "1  9333dfbd-b87a-4708-9857-6336556b0fcc  \n",
            "2  566511c2-e2e3-422b-b695-cf8e6e792ca8  \n",
            "3  7c90fc70-0e80-4590-96f3-13c02c18c713  \n",
            "4  c6bd7419-2748-4c56-95b4-8cec9ff8b80d  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# تغيير اسم الملف حسب الموجود فعلاً في مجلدك\n",
        "csv_file = \"/content/2019-Oct.csv\"\n",
        "\n",
        "# نقرأ أول 5 صفوف فقط للتحقق من البنية\n",
        "sample_df = pd.read_csv(csv_file, nrows=5)\n",
        "print(\"معاينة أولية للبيانات:\")\n",
        "print(sample_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_RIsPhvKuF9",
        "outputId": "9aeaa256-cc71-4cc3-ae7e-74d25d6a5ef8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "عدد الصفوف الكلي: 42448764\n",
            "الوقت المستغرق: 84.92 ثانية باستخدام chunksize\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "chunk_size = 30000  # عدد الصفوف في كل جزء (يمكن تغييره)\n",
        "chunks = []\n",
        "\n",
        "for chunk in pd.read_csv(csv_file, chunksize=chunk_size):\n",
        "    # مثال: نحسب عدد الصفوف فقط دون تحميل كل البيانات في الذاكرة\n",
        "    chunks.append(len(chunk))\n",
        "\n",
        "total_rows = sum(chunks)\n",
        "elapsed_chunks = time.time() - start_time\n",
        "\n",
        "print(f\"عدد الصفوف الكلي: {total_rows}\")\n",
        "print(f\"الوقت المستغرق: {elapsed_chunks:.2f} ثانية باستخدام chunksize\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyuCBeq5MQoK",
        "outputId": "38908f34-3e93-44c9-bb1f-f2495c095daf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dask in /usr/local/lib/python3.12/dist-packages (2025.5.0)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.12/dist-packages (from dask) (8.3.0)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from dask) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.12/dist-packages (from dask) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from dask) (25.0)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from dask) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from dask) (6.0.3)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from dask) (0.12.1)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.12/dist-packages (from partd>=1.4.0->dask) (1.0.0)\n",
            "عدد الصفوف: 42448764\n",
            "متوسط السعر: 290.32\n",
            "توزيع الأحداث:\n",
            "event_type\n",
            "view        40779399\n",
            "cart          926516\n",
            "purchase      742849\n",
            "Name: count, dtype: int64[pyarrow]\n",
            "الوقت المستغرق بـ Dask: 292.67 ثانية\n"
          ]
        }
      ],
      "source": [
        "!pip install dask\n",
        "\n",
        "import dask.dataframe as dd\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# نقرأ الملف باستخدام Dask\n",
        "df_dask = dd.read_csv(csv_file)\n",
        "\n",
        "# نقوم بعمليات إحصائية\n",
        "row_count = df_dask.shape[0].compute()  # عدد الصفوف\n",
        "average_price = df_dask['price'].mean().compute()  # متوسط السعر\n",
        "event_counts = df_dask['event_type'].value_counts().compute()  # عدد كل نوع حدث\n",
        "\n",
        "elapsed_dask = time.time() - start_time\n",
        "\n",
        "print(f\"عدد الصفوف: {row_count}\")\n",
        "print(f\"متوسط السعر: {average_price:.2f}\")\n",
        "print(f\"توزيع الأحداث:\")\n",
        "print(event_counts)\n",
        "print(f\"الوقت المستغرق بـ Dask: {elapsed_dask:.2f} ثانية\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnL-ymkTN6SU",
        "outputId": "682b27ca-dabe-4c02-b68c-6ae45357610f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⏱️ وقت الضغط: 437.10 ثانية\n",
            "⏱️ وقت القراءة بعد الضغط: 0.20 ثانية\n",
            "🗜️ حجم الملف قبل وبعد الضغط:\n",
            "-rw-r--r-- 1 root root 8.4G Oct 15 09:18 2019-Nov.csv\n",
            "-rw-r--r-- 1 root root 5.3G Oct 15 09:19 2019-Oct.csv\n",
            "-rw-r--r-- 1 root root 1.6G Oct 15 09:40 data_compressed.csv.gz\n"
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "# ضغط الملف إلى gzip\n",
        "start_compress = time.time()\n",
        "with open(csv_file, 'rb') as f_in:\n",
        "    with gzip.open('data_compressed.csv.gz', 'wb') as f_out:\n",
        "        shutil.copyfileobj(f_in, f_out)\n",
        "compress_time = time.time() - start_compress\n",
        "\n",
        "# قراءة الملف المضغوط باستخدام Pandas\n",
        "start_read = time.time()\n",
        "df_compressed = pd.read_csv('data_compressed.csv.gz', compression='gzip', nrows=50000)\n",
        "read_time = time.time() - start_read\n",
        "\n",
        "print(f\"⏱️ وقت الضغط: {compress_time:.2f} ثانية\")\n",
        "print(f\"⏱️ وقت القراءة بعد الضغط: {read_time:.2f} ثانية\")\n",
        "print(f\"🗜️ حجم الملف قبل وبعد الضغط:\")\n",
        "!ls -lh *.csv*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sYywKM_dPo6K",
        "outputId": "c3bd5788-4d5c-4afb-a3ce-a8af540a15a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "📊 Big Data Processing Results - Basic Comparison\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Method</th>\n",
              "      <th>Approximate Time (seconds)</th>\n",
              "      <th>Notes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pandas + chunksize</td>\n",
              "      <td>85.0</td>\n",
              "      <td>Memory-safe but slow in processing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dask</td>\n",
              "      <td>85.0</td>\n",
              "      <td>Fast and supports distributed processing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Pandas + Compression</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Saves space but requires initial compression time</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Method  Approximate Time (seconds)  \\\n",
              "0    Pandas + chunksize                        85.0   \n",
              "1                  Dask                        85.0   \n",
              "2  Pandas + Compression                         0.2   \n",
              "\n",
              "                                               Notes  \n",
              "0                 Memory-safe but slow in processing  \n",
              "1           Fast and supports distributed processing  \n",
              "2  Saves space but requires initial compression time  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[WinError 3] The system cannot find the path specified: '/content/2019-Oct.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     23\u001b[39m results = []\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Pandas Chunking\u001b[39;00m\n\u001b[32m     26\u001b[39m results.append({\n\u001b[32m     27\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmethod\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mPandas Chunking\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     28\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtime_s\u001b[39m\u001b[33m'\u001b[39m: elapsed_chunks,\n\u001b[32m     29\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrows\u001b[39m\u001b[33m'\u001b[39m: total_rows,\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mstorage_GB\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetsize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m)\u001b[49m / (\u001b[32m1024\u001b[39m**\u001b[32m3\u001b[39m),\n\u001b[32m     31\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mefficiency\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mMedium\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     32\u001b[39m })\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Dask\u001b[39;00m\n\u001b[32m     35\u001b[39m results.append({\n\u001b[32m     36\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmethod\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mDask\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     37\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtime_s\u001b[39m\u001b[33m'\u001b[39m: elapsed_dask,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mefficiency\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mHigh\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     41\u001b[39m })\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen genericpath>:50\u001b[39m, in \u001b[36mgetsize\u001b[39m\u001b[34m(filename)\u001b[39m\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: '/content/2019-Oct.csv'"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# ====== Basic Results ======\n",
        "results_arabic = pd.DataFrame({\n",
        "    'Method': ['Pandas + chunksize', 'Dask', 'Pandas + Compression'],\n",
        "    'Approximate Time (seconds)': [elapsed_chunks, elapsed_dask, read_time],\n",
        "    'Notes': [\n",
        "        'Memory-safe but slow in processing',\n",
        "        'Fast and supports distributed processing',\n",
        "        'Saves space but requires initial compression time'\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"📊 Big Data Processing Results - Basic Comparison\")\n",
        "print(\"=\" * 60)\n",
        "display(results_arabic)\n",
        "\n",
        "# ====== Advanced Comparison ======\n",
        "# Collect results for advanced comparison\n",
        "results = []\n",
        "\n",
        "# Pandas Chunking\n",
        "results.append({\n",
        "    'method': 'Pandas Chunking',\n",
        "    'time_s': elapsed_chunks,\n",
        "    'rows': total_rows,\n",
        "    'storage_GB': os.path.getsize(csv_file) / (1024**3),\n",
        "    'efficiency': 'Medium'\n",
        "})\n",
        "\n",
        "# Dask\n",
        "results.append({\n",
        "    'method': 'Dask',\n",
        "    'time_s': elapsed_dask,\n",
        "    'rows': row_count,\n",
        "    'storage_GB': os.path.getsize(csv_file) / (1024**3),\n",
        "    'efficiency': 'High'\n",
        "})\n",
        "\n",
        "# Compression (if data is available)\n",
        "try:\n",
        "    compressed_size = os.path.getsize('data_compressed.csv.gz') / (1024**3) if os.path.exists('data_compressed.csv.gz') else 0\n",
        "    results.append({\n",
        "        'method': 'Compression',\n",
        "        'time_s': read_time,\n",
        "        'rows': 50000,  # Number of rows we read in the example\n",
        "        'storage_GB': compressed_size,\n",
        "        'efficiency': 'Very High'\n",
        "    })\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Create DataFrame for results\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# Calculate storage saving percentage\n",
        "csv_size = os.path.getsize(csv_file) / (1024**3)\n",
        "df_results['storage_saving'] = ((csv_size - df_results['storage_GB']) / csv_size * 100).round(2)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"📊 Advanced Comparison - Big Data Processing Methods\")\n",
        "print(\"=\" * 60)\n",
        "display(df_results)\n",
        "\n",
        "# ====== Charts ======\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Chart 1: Processing Time Comparison\n",
        "plt.subplot(2, 2, 1)\n",
        "methods = df_results['method']\n",
        "times = df_results['time_s']\n",
        "plt.bar(methods, times, color=['skyblue', 'lightgreen', 'orange'])\n",
        "plt.title('Processing Time Comparison (Seconds)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Time (Seconds)')\n",
        "\n",
        "# Chart 2: Storage Usage Comparison\n",
        "plt.subplot(2, 2, 2)\n",
        "storage = df_results['storage_GB']\n",
        "plt.bar(methods, storage, color=['skyblue', 'lightgreen', 'orange'])\n",
        "plt.title('Storage Usage Comparison (Gigabytes)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Size (GB)')\n",
        "\n",
        "# Chart 3: Storage Saving Percentage\n",
        "plt.subplot(2, 2, 3)\n",
        "savings = df_results['storage_saving']\n",
        "plt.bar(methods, savings, color=['gray', 'gray', 'green'])\n",
        "plt.title('Storage Saving Percentage (%)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Saving %')\n",
        "\n",
        "# Chart 4: Efficiency Comparison\n",
        "plt.subplot(2, 2, 4)\n",
        "efficiency_map = {'Medium': 2, 'High': 3, 'Very High': 4, 'Excellent': 5}\n",
        "efficiency_scores = [efficiency_map[eff] for eff in df_results['efficiency']]\n",
        "plt.bar(methods, efficiency_scores, color=['yellow', 'orange', 'lightgreen'])\n",
        "plt.title('Overall Efficiency Comparison')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Efficiency Level')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/comparison_results.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# ====== Final Summary ======\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"📋 Final Summary\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"💾 Original File Size (CSV): {csv_size:.2f} GB\")\n",
        "print(f\"📊 Total Rows: {total_rows:,}\")\n",
        "print(f\"⚡ Fastest Method: {df_results.loc[df_results['time_s'].idxmin(), 'method']}\")\n",
        "print(f\"💾 Most Space-Efficient Method: {df_results.loc[df_results['storage_saving'].idxmax(), 'method']}\")\n",
        "\n",
        "# Display Arabic results again\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"📊 Final Results in Arabic\")\n",
        "print(\"=\" * 60)\n",
        "display(results_arabic)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
